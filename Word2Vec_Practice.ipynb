{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483000e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin by implementing the simple model provided by the author\n",
    "# copying from https://github.com/Eligijus112/word-embedding-creation/\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4f6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning: more dependencies\n",
    "# luckily all the required packages were already installed in my mlenv anaconda environment used in class\n",
    "\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# Custom functions- my own version saved in my local folder\n",
    "from utils import text_preprocessing, create_unique_word_dict, clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4df224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where the real code begins \n",
    "\n",
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "# restore later after I play around with manually introducing new text\n",
    "texts = [x for x in texts['text']]\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "                \n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53f954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# for simplicity and debugging I am splitting up the code into cells that can be run individually\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "\n",
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1be656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1.0\n",
      "  (1, 6)\t1.0\n",
      "  (2, 7)\t1.0\n",
      "  (3, 7)\t1.0\n",
      "  (4, 11)\t1.0\n",
      "  (5, 11)\t1.0\n",
      "  (6, 4)\t1.0\n",
      "  (7, 12)\t1.0\n",
      "  (8, 17)\t1.0\n",
      "  (9, 11)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 10)\t1.0\n",
      "  (12, 8)\t1.0\n",
      "  (13, 8)\t1.0\n",
      "  (14, 8)\t1.0\n",
      "  (15, 2)\t1.0\n",
      "  (16, 2)\t1.0\n",
      "  (17, 2)\t1.0\n",
      "  (18, 7)\t1.0\n",
      "  (19, 7)\t1.0\n",
      "  (20, 10)\t1.0\n",
      "  (21, 10)\t1.0\n",
      "  (22, 20)\t1.0\n",
      "  (23, 20)\t1.0\n",
      "  (24, 20)\t1.0\n",
      "  :\t:\n",
      "  (59, 7)\t1.0\n",
      "  (60, 7)\t1.0\n",
      "  (61, 7)\t1.0\n",
      "  (62, 7)\t1.0\n",
      "  (63, 13)\t1.0\n",
      "  (64, 13)\t1.0\n",
      "  (65, 13)\t1.0\n",
      "  (66, 13)\t1.0\n",
      "  (67, 19)\t1.0\n",
      "  (68, 19)\t1.0\n",
      "  (69, 19)\t1.0\n",
      "  (70, 3)\t1.0\n",
      "  (71, 3)\t1.0\n",
      "  (72, 11)\t1.0\n",
      "  (73, 11)\t1.0\n",
      "  (74, 10)\t1.0\n",
      "  (75, 10)\t1.0\n",
      "  (76, 10)\t1.0\n",
      "  (77, 1)\t1.0\n",
      "  (78, 1)\t1.0\n",
      "  (79, 1)\t1.0\n",
      "  (80, 9)\t1.0\n",
      "  (81, 9)\t1.0\n",
      "  (82, 1)\t1.0\n",
      "  (83, 8)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# lets inspect the contents of the variables before moving to the next steps \n",
    "# looks like the X variable has 1355 data points\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the next chunk of code to make sure it runs \n",
    "# we get an error from the model.fit so lets walk it through 1 line at a time\n",
    "\n",
    "# STOP HERE FOR NOW UNTIL I COMPLETE MORE TROUBLESHOOTING \n",
    "\n",
    "# Defining the size of the embedding\n",
    "embed_size = 2\n",
    "\n",
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get an error here so maybe its the version of keras or maybe something did not install?\n",
    "# I am going to go back to using the provided sample.csv to train the model see if that has better luck\n",
    "# version of tensorflow required should be 4.43.0 but my mlenv environment has version 4.59.0 - could that be the reason?\n",
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )\n",
    "\n",
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })\n",
    "\n",
    "# Ploting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))       \n",
    "\n",
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')        \n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder: {e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt', 'w') as f:\n",
    "    for key, value in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')   \n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
