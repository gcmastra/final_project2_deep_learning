# final_project2_deep_learning
### Student: Christopher Mastrangelo

## Neural Network word pattern recognition using Word2Vec for Python

This project will attempt to answer questions like these: "Why do spell checkers never work for me?"  

Why is it when I text people on my phone, the suggested word never guesses what I want to say next?  Maybe not all the time, but it seems like more often than not, the word suggestions generated by an AI algorithm guess wrong.  Is it because they are trained on classic English writing sources rather than a writing style that matches my own?

Would it be possible to use examples of my own writing to train a machine learning (unsupervised neural network) word embedding algorithm to generate the list of next words? 

If I train the neural network on examples of my own writing could I get a better guess for the next suggested word? 

Word2Vec converts words into decimal number representations because neural networks only work on numeric data. In addition to creating a word embedding algorithm, the neural network uses word "vectors" to group words into associations.  Words that are similar should have a shorter "distance" between their respective vectors as shown in the example.

![image](https://user-images.githubusercontent.com/86205000/142961015-928aa068-3485-4554-8b37-2b33955335de.png)

What information can be learned by analyzing a vector diagram of my own writing?  Are some words more common?  Do I use  different vocabulary than the general English speaking population?  Are the associations between words logical?  Are any of the words clustered together and if so are any of the associations unexpected?

<hr>

### Data Source - sentences from my own email collection.

Code source- the python code for creating and training a model already exists in this GitHub repository

https://github.com/Eligijus112/word-embedding-creation

which is owned by the author (Eligijus Bujokas) of an article which explains how to build the model step  by step on a relatively small set of sentences. Here is a link to the article:

https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8



<b>ETL</b> might be a little challenging - use web scraping to pull in text from emails and tokenize it into sentences so it can be fed in one sentence at a time to the algorithm.

<hr>
<b>Proposal</b>- after getting the model from the article working, then train a new copy of the model using sentences taken from examples of my own writing. 

<b>Visualizations</b>- create a 2-dimensional model of the word vectors showing groupings of associated words

<b>Implementation</b> - after training the model, determine how to apply it in "real life" by implementing a simple interactive "next word guessing" program in Python.

<hr>

## Location of files

Explanation of files found in the main branch of the repository
- <b>utils.py</b> contains python functions provided by the author of the article from his repository linked above
- <b>Word2Vec_Practice.ipynb</b> is a Jupyter Notebook file containing code copies from the author's master.py file and modified to break it up into manageable size chunls to run in notebook cells to make troubleshooting easier and to be able to print the contents of variable at different steps in the process
- the <b>input folder</b> contains the original sample.csv used to train the model in the example, as well as one test version of text file taken from my own email

### FOR THE GRADER
- I am going to submit this with a link to the new repository I created so you have a chance to look at it for Tuesday night 
- This should count for deliverable 1 and 2 at this point I think?  I submitted deliverable 1 separately based on email from Stephanie who should also talk to you about it.
- I am not sure if I will use branches in GitHub since I am working on this myself.  However, just to check the box in the rubric, I could create a separate branch for the test I will run using my own text as input instead of the canned training data
- In the near future I will add more notebook files after debugging the original algorithm, and link their output to an interactive dashboard
