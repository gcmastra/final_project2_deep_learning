{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb2f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to the requirements.txt - gensim==3.8.3 and sklearm\n",
    "# Using this copy to test a few more sample inputs to the word association functions in the KeyedVectors object model\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec,KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# I installed gensim and sklearn in the final_project anaconda environment so now everything loads\n",
    "# gensim version has to be 3.8.3 to work with code below - the latest version 4.1.2 causes an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import in word2vec format leave the binary option out of the function call \n",
    "# make sure to add the trained folder to the .gitignore so it does not try to upload it to GitHub\n",
    "\n",
    "# filename = './trained/glove.6B.100d.txt'\n",
    "filename = './output/astro2_out_vectors.txt'\n",
    "# vector_word_notations = KeyedVectors.load_word2vec_format(filename,binary=False)\n",
    "\n",
    "# try #2 from instructions in https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "# must convert from glove format to word2vec format before it can be imported?\n",
    "glove_file = \"./trained/glove.6B.100d.txt\"\n",
    "tmp_file = \"./trained/test_word2vec.txt\"\n",
    "\n",
    "# next line rebuilds the entire conversion file, but we already have it saved in the ./trained/ folder\n",
    "# so do not run this line again unless you want to wait 30 minutes\n",
    "# this cell might still take some time because the tmp_file is the same size as the original = 338 Mb (1/3 Gig)\n",
    "# _ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "vector_word_notations = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79eb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at variables and attributes of KeyedVectors.object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ae3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous cell never seems to reach the end even though the file stopped growing. \n",
    "# maybe the output file is still open for writing and is just sitting in limbo?\n",
    "# finally got a number in the cell so it appears to have caught up\n",
    "\n",
    "# create this function found in URL \n",
    "# https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/\n",
    "# then use it in the next cell to see if you get results \n",
    "\n",
    "def analogous_word(word_1,word_2,word_3,vector_word_notations):\n",
    "#     ''' The function accepts a triad of words, word_1, word_2, word_3 \n",
    "# and returns word_4 such that word_1:word_2::word_3:word_4 '''\n",
    "      \n",
    "    # converting each word to its lowercase\n",
    "    word_1,word_2,word_3 = word_1.lower(),word_2.lower(),word_3.lower()\n",
    "      \n",
    "    # Similarity between |word_2-word_1| = |word_4-word_3| should be maximum\n",
    "    maximum_similarity = -99999\n",
    "      \n",
    "    word_4 = None\n",
    "      \n",
    "    words = vector_word_notations.vocab.keys()\n",
    "      \n",
    "    va,vb,vc = vector_word_notations[word_1],\\\n",
    "    vector_word_notations[word_2],vector_word_notations[word_3]\n",
    "      \n",
    "    # to find word_4 such that similarity\n",
    "    # (|word_2 - word_1|, |word_4 - word_3|) should be maximum\n",
    "    \n",
    "    for i in words:\n",
    "        if i in [word_1,word_2,word_3]:\n",
    "            continue\n",
    "          \n",
    "        wvec = vector_word_notations[i]\n",
    "        similarity = cosine_similarity([wvec-vc])\n",
    "        # the next line had a syntax error\n",
    "        # similarity = cosine_similarity(,[wvec-vc])\n",
    "        \n",
    "          \n",
    "        if similarity > maximum_similarity:\n",
    "            maximum_similarity = similarity\n",
    "            word_4 = i   \n",
    "            # remove after debug - only do this once\n",
    "            print(i, similarity)\n",
    "  \n",
    "    return word_4\n",
    "\n",
    "# when I added the print to debug, I should only have printed the words that replace the maximum so we see it zooming in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5134639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test different words, recommend you verify that the word is in the vector file before selecting it\n",
    "# it would be rare for common words not to be included but I recommend to check\n",
    "\n",
    "# word1\n",
    "word6 = 'june'\n",
    "word5 = 'july'\n",
    "vec5 = vector_word_notations[word5]\n",
    "vec6 = vector_word_notations[word6]\n",
    "print(word5, vec5)\n",
    "print(word6, vec6)\n",
    "cosine_test = cosine_similarity([vec6-vec5])\n",
    "print(cosine_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next line actually works but let's do it the same way it is done in the web page\n",
    "# code is from URL https://www.geeksforgeeks.org/finding-the-word-analogy-from-given-words-using-word2vec-embeddings/\n",
    "# this takes a while because it is a big 100 dimensional file\n",
    "\n",
    "# try # 1 -> word1,word2,word3=(\"king\",\"man\",\"woman\")\n",
    "# answer # 1 = '1.94' which is a number not a word but it was in the vector file\n",
    "# ape, man, dog - the answer is 'then' - wow all that work for this crap?\n",
    "\n",
    "word1,word2,word3=(\"june\",\"wedding\",\"december\")\n",
    "output = analogous_word(word1, word2, word3, vector_word_notations) \n",
    "\n",
    "print(output)\n",
    "# answer is a number or a word ? I thought it would be a word\n",
    "# trick question - the answer '1.94' IS A WORD in the vector file - it has some numeric values as text - disappointing\n",
    "# that means I have to do it again with different  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting fact\n",
    "# the words in this dataset are non standard words, they were returned from a web crawler algorithm\n",
    "# the list of words might make an interesting slide to show how crazy some of them are\n",
    "vector_word_notations.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8076ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word1\n",
    "word5 = 'boat'\n",
    "print(word5, vector_word_notations[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca848bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = vector_word_notations[word1]\n",
    "vec2 = vector_word_notations[word2]\n",
    "answer = cosine_similarity([vec2-vec1])\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "final_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
